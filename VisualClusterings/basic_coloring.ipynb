{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def tokenize_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().lower()\n",
    "    # Remove accents\n",
    "    content = remove_accents(content)\n",
    "    # Split by comma or newline, then strip whitespace\n",
    "    tokens = [token.strip() for token in re.split(r'[,\\n]', content) if token.strip()]\n",
    "    return tokens\n",
    "\n",
    "# Read and tokenize both files\n",
    "names = tokenize_file('name_segregated_tokens.txt')\n",
    "places = tokenize_file('place_segregated_tokens.txt')\n",
    "\n",
    "# Remove duplicates\n",
    "names = list(set(names))\n",
    "places = list(set(places))\n",
    "\n",
    "# Move common tokens from places to names\n",
    "common_tokens = set(names) & set(places)\n",
    "names.extend(common_tokens)\n",
    "places = [place for place in places if place not in common_tokens]\n",
    "\n",
    "# Sort the lists\n",
    "names.sort()\n",
    "places.sort()\n",
    "\n",
    "print(f\"Number of unique names: {len(names)}\")\n",
    "print(f\"Number of unique places: {len(places)}\")\n",
    "\n",
    "# Optional: Print the first few elements of each list to verify\n",
    "print(\"\\nFirst 10 names:\")\n",
    "print(names[:10])\n",
    "print(\"\\nFirst 10 places:\")\n",
    "print(places[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../unidades.csv')\n",
    "\n",
    "# Display the first few rows to verify data\n",
    "df.head()\n",
    "\n",
    "# Define a function to remove accents\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# Define synonyms\n",
    "synonyms = {\n",
    "    'gab': 'gabinete',\n",
    "    'gab.': 'gabinete',\n",
    "    'presidencia': 'presidencia',\n",
    "    'v': 'vara',\n",
    "    'var': 'vara',\n",
    "    'vio': 'violencia',\n",
    "    'c': 'circunscricao',\n",
    "    'juiza': 'juiz',\n",
    "    'substituta': 'substituto',\n",
    "    'dra': 'dr',\n",
    "    'faz': 'fazenda',\n",
    "    'fam': 'familia',\n",
    "    'exma': 'exmo',\n",
    "    'reg': 'registros',\n",
    "    'pub': 'publico',\n",
    "    'publ': 'publico',\n",
    "    'publica': 'publico',\n",
    "    'juv': 'juventude',\n",
    "    'inf': 'infancia',\n",
    "    'crim': 'criminal',\n",
    "    'DEECRIM': 'criminal',\n",
    "    'adj': 'adjunto',\n",
    "    'cons': 'consumo',\n",
    "    'jef': 'federal',\n",
    "    'jud': 'judiciario',\n",
    "    'desembargadora': 'desembargador',\n",
    "    'desebargadora': 'desembargador',\n",
    "    'desembargado': 'desembargador',\n",
    "    'desembargador': 'desembargador',\n",
    "    'desembargadores': 'desembargador',\n",
    "    'des': 'desembargador',\n",
    "    'desa': 'desembargador',\n",
    "    'desemb': 'desembargador',\n",
    "    'j': 'juizado',\n",
    "    'jui': 'juizado',\n",
    "    'civ': 'civel',\n",
    "    'civeis': 'civel',\n",
    "    'civil': 'civel',\n",
    "    'crminal' : 'criminal',\n",
    "    'esp': 'especial',\n",
    "    'especiais': 'especial',\n",
    "    'educativa': 'educacional',\n",
    "    'contadoria/tesouraria': 'contadoria',\n",
    "    'c/mulher': 'mulher',\n",
    "    'calculos': 'calculo',\n",
    "    'calc': 'calculo',\n",
    "    'mulh': 'mulher',\n",
    "    'adm': 'administracao',\n",
    "    'amb': 'ambiental',\n",
    "    'acomp': 'acompanhamento',\n",
    "    'aten': 'atencao',\n",
    "    'atend': 'atendimento',\n",
    "    'aux': 'auxiliar',\n",
    "    'aval': 'avaliacao',\n",
    "    'compet': 'competencia',\n",
    "    'conf': 'conflito',\n",
    "    'confl': 'conflito',\n",
    "    'coord': 'coordenacao',\n",
    "    'cump': 'cumprimento',\n",
    "    'def': 'defensoria',\n",
    "    'dep': 'departamento',\n",
    "    'dist': 'distribuicao',\n",
    "    'distr': 'distribuicao',\n",
    "    'exec': 'execucao',\n",
    "    'fisc': 'fiscal',\n",
    "    'gest': 'gestao',\n",
    "    'inform': 'informacao',\n",
    "    'inq': 'inquerito',\n",
    "    'jurid': 'juridico',\n",
    "    'med': 'mediacao',\n",
    "    'mun': 'municipal',\n",
    "    'munic': 'municipal',\n",
    "    'org': 'organizacao',\n",
    "    'pres': 'presidencia',\n",
    "    'proc': 'processo',\n",
    "    'prog': 'programa',\n",
    "    'proj': 'projeto',\n",
    "    'prot': 'protocolo',\n",
    "    'rec': 'recurso',\n",
    "    'rel': 'relator',\n",
    "    'sec': 'secretaria',\n",
    "    'serv': 'servico',\n",
    "    'sist': 'sistema',\n",
    "    'tec': 'tecnico',\n",
    "    'trib': 'tribunal',\n",
    "    'unid': 'unidade',\n",
    "    'fg' : 'fig',\n",
    "    'gm' : 'gmf',\n",
    "    '[microrregiao' : 'microrregiao',\n",
    "    'centr': 'centro',\n",
    "    'turmas': 'turma',\n",
    "    'tributaria': 'tributarios', \n",
    "    'trt9': 'trt',\n",
    "    'acidentes': 'acidente',\n",
    "    'administracao': 'administrativo',\n",
    "    'administrativa': 'administrativo',\n",
    "    'zonas' : 'zona',\n",
    "    '*decima': 'decima',\n",
    "    'acervos': 'acervo',\n",
    "    'acordaos': 'acordao',\n",
    "    'adjunta': 'adjunto',\n",
    "    'administrativas': 'administrativo',\n",
    "    'administrativos': 'administrativo',\n",
    "    'adolescentes': 'adolescente',\n",
    "    'alta vara': 'alto vara',\n",
    "    'anexos': 'anexo',\n",
    "    '\\\\gabinete' : 'gabinete',\n",
    "    'detrabalho' : 'trabalho',\n",
    "    'conflitos' : 'conflito',\n",
    "    'cidanania' : 'cidadania',\n",
    "    'arquivos' : 'arquivo',\n",
    "\n",
    "    # Add more synonyms as needed\n",
    "}\n",
    "\n",
    "# Define multi-token replacements\n",
    "multi_token_replacements = {\n",
    "    'vt': ['vara', 'trabalho'],\n",
    "    'cejusc' :['centro','judicial','solucao','conflitos','cidadania'],\n",
    "    'tr': ['turma', 'recursal'],\n",
    "    'jit': ['juizado', 'especial','civel'],\n",
    "    'gades': ['gabinete', 'desembargador'],\n",
    "    'jesp': ['juizado', 'especial','criminal'],\n",
    "    'jec': ['juizado', 'especial','civel'],\n",
    "    'saf': ['servico', 'anexo','fazendas'],\n",
    "    'aos, autos' : ['aos', 'autos'],\n",
    "    # Add more multi-token replacements as needed\n",
    "}\n",
    "\n",
    "# Function to replace synonyms and multi-token replacements\n",
    "def replace_synonyms_and_multi_tokens(token):\n",
    "    if token in multi_token_replacements:\n",
    "        return multi_token_replacements[token]\n",
    "    else:\n",
    "        return [synonyms.get(token, token)]\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_name(name, additional_stopwords=None):\n",
    "    name = remove_accents(name).lower()\n",
    "    \n",
    "    # REPLACE 'CJ' OR 'C J' WITH 'CIRCUNSCRICAO JUDICIAL'\n",
    "    name = re.sub(r'\\bc\\s*j\\b', 'circunscricao', name)\n",
    "    \n",
    "    # REMOVE NUMBERS AND NUMBER-LETTER COMBINATIONS, BUT KEEP THE PRECEDING WORD\n",
    "    name = re.sub(r'\\b(\\d+\\w*)\\b', '', name)\n",
    "    \n",
    "    # COMBINE 'VICE' WITH THE FOLLOWING WORD (WITH SPACE OR HYPHEN) INTO A SINGLE TOKEN\n",
    "    name = re.sub(r'\\b(vice)[-\\s]+(\\w+)', r'\\1_\\2', name)\n",
    "\n",
    "    \n",
    "    tokens = re.split(r'\\s|,|\\.|\\(|\\)|-|\\{|\\}|\\[|\\]|\\'|\\\"', name)\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]\n",
    "\n",
    "    stopwords = ['de', 'da', 'do', 'das', 'dos', \n",
    "                 'e', 'a', 'o', 'i', 'u', 'b', 'as', 'ao',\n",
    "                 '\"', 'em', 'des', 'com', 'n', 'g', 'ap', 'sr', 'sra','/', '\\\\' ,'?', '\\'', '\\\\\\'', '\\\"gabinete', 'ou',\n",
    "                'hora', 'solteira', 'villa','zz', '°', '¿',\n",
    "                'i','ii','iii','iv','v','vi','vii','viii','ix','x',\n",
    "                'xi','xii','xiii','xiv','xv','xvi','xvii','xviii','xix','xx',\n",
    "                'xxi','xxii','xxiii','xxiv','xxv','xxvi','xxvii','xxviii','xxix','xxx',\n",
    "                'xxxi','xxxii','xxxiii','xxxiv','xxxv','xxxvi','xxxvii','xxxviii','xxxix','xl',\n",
    "                'xli','xlii','xliii','xliv','xlv','xlvi','xlvii','xlviii','xlix','l',\n",
    "                'li','lii','liii','liv','lv','lvi','lvii','lviii','lix','lx',\n",
    "                'lxi','lxii','lxiii','lxiv','lxv','lxvi','lxvii','lxviii','lxix','lxx',\n",
    "                'lxxi','lxxii','lxxiii','lxxiv','lxxv','lxxvi','lxxvii','lxxviii','lxxix','lxxx',\n",
    "                'lxxxi','lxxxii','lxxxiii','lxxxiv','lxxxv','lxxxvi','lxxxvii','lxxxviii','lxxxix','xc',\n",
    "                'xci','xcii','xciii','xciv','xcv','xcvi','xcvii','xcviii','xcix','c','sanclerlandia','goianapolis','?',':','ci','cii','varao']\n",
    "    if additional_stopwords:\n",
    "        stopwords.extend(additional_stopwords)\n",
    "    \n",
    "\n",
    "    def combine_words(name, stopwords):\n",
    "        # Define the words to be combined\n",
    "        words_to_combine = [\n",
    "            'sao', 'santa', 'santo', 'nova', 'novo', 'bom', 'boa', \n",
    "            'alto', 'alta', 'baixo', 'baixa', 'porto', 'campos', \n",
    "            'rio', 'foz', 'barra', 'passa', 'entre'\n",
    "        ]\n",
    "\n",
    "        # Function to replace word and its following non-stopword\n",
    "        def replace_word(match):\n",
    "            word1 = match.group(1)\n",
    "            word2 = match.group(2)\n",
    "            if word2.lower() not in stopwords:\n",
    "                return f'{word1}_{word2}'\n",
    "            return f'{word1} {word2}'\n",
    "\n",
    "        # Create a combined regex pattern for all words to be combined\n",
    "        pattern = r'\\b(' + '|'.join(words_to_combine) + r')[\\s-]+(\\w+)'\n",
    "\n",
    "        # Apply the replacement\n",
    "        name = re.sub(pattern, replace_word, name)\n",
    "\n",
    "        return name\n",
    "    \n",
    "    name = combine_words(name, stopwords)\n",
    "\n",
    "    stopwords.extend([name.lower() for name in names])\n",
    "\n",
    "    if additional_stopwords:\n",
    "        stopwords.extend(additional_stopwords)\n",
    "\n",
    "    tokens = re.split(r'\\s|,|\\.|\\(|\\)|-', name)\n",
    "    tokens = [token.strip() for token in tokens if token.strip() and token not in stopwords]\n",
    "\n",
    "    \n",
    "    # PROCESS EACH TOKEN, APPLYING SYNONYMS AND MULTI-TOKEN REPLACEMENTS\n",
    "    processed_tokens = []\n",
    "    skip_next = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        if token and token not in stopwords:\n",
    "            # HANDLE CASE WHERE 'C' IS FOLLOWED BY 'J'\n",
    "            if token == 'c' and i + 1 < len(tokens) and tokens[i + 1] == 'j':\n",
    "                processed_tokens.append('circunscricao')\n",
    "                skip_next = True\n",
    "            else:\n",
    "                processed_tokens.extend(replace_synonyms_and_multi_tokens(token))\n",
    "    \n",
    "    # REPLACE UNDERSCORES WITH SPACES IN PRESERVED CONJOINED EXPRESSIONS\n",
    "    processed_tokens = [token.replace('_', ' ') for token in processed_tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# Apply tokenization and store original names\n",
    "df['tokens'] = df['nomeUnidade'].apply(lambda x: tokenize_name(x))\n",
    "df['original_name'] = df['nomeUnidade']\n",
    "\n",
    "# Flatten list of tokens\n",
    "all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "\n",
    "# Calculate frequency of each token\n",
    "token_counts = Counter(all_tokens)\n",
    "common_tokens = token_counts.most_common()\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "token_df = pd.DataFrame(common_tokens, columns=['token', 'count'])\n",
    "\n",
    "print(f\"Number of disparate tokens: {len(set(all_tokens))}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "token_df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    return Levenshtein.distance(s1, s2)\n",
    "\n",
    "def find_similar_tokens(tokens, threshold=0.8):\n",
    "    similar_tokens = defaultdict(set)\n",
    "    for i, token1 in enumerate(tokens):\n",
    "        for token2 in tokens[i+1:]:\n",
    "            if token1 != token2:\n",
    "                max_len = max(len(token1), len(token2))\n",
    "                similarity = 1 - (levenshtein_distance(token1, token2) / max_len)\n",
    "                if similarity >= threshold:\n",
    "                    similar_tokens[token1].add(token2)\n",
    "                    similar_tokens[token2].add(token1)\n",
    "    return similar_tokens\n",
    "\n",
    "def group_similar_tokens(similar_tokens):\n",
    "    groups = []\n",
    "    processed_tokens = set()\n",
    "\n",
    "    for token, similar in similar_tokens.items():\n",
    "        if token not in processed_tokens:\n",
    "            group = {token} | similar\n",
    "            groups.append(group)\n",
    "            processed_tokens |= group\n",
    "\n",
    "    return groups\n",
    "\n",
    "def consolidate_tokens(groups, token_counts):\n",
    "    token_mapping = {}\n",
    "    for group in groups:\n",
    "        most_common = max(group, key=lambda t: token_counts[t])\n",
    "        for token in group:\n",
    "            token_mapping[token] = most_common\n",
    "    return token_mapping\n",
    "\n",
    "def apply_token_mapping(tokens, token_mapping):\n",
    "    return [token_mapping.get(token, token) for token in tokens]\n",
    "\n",
    "# Use existing token counts\n",
    "token_counts = token_df['token'].value_counts()\n",
    "\n",
    "# Get unique tokens for similarity comparison\n",
    "unique_tokens = token_df['token'].unique().tolist()\n",
    "\n",
    "# Find similar tokens among all unique tokens\n",
    "similar_tokens = find_similar_tokens(unique_tokens)\n",
    "\n",
    "# Group similar tokens\n",
    "grouped_tokens = group_similar_tokens(similar_tokens)\n",
    "\n",
    "# Consolidate tokens\n",
    "token_mapping = consolidate_tokens(grouped_tokens, token_counts)\n",
    "\n",
    "# Store original tokens\n",
    "df['original_tokens'] = df['tokens']\n",
    "\n",
    "# Replace original tokens with consolidated tokens\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: apply_token_mapping(tokens, token_mapping))\n",
    "\n",
    "# Update token_df with consolidated tokens\n",
    "consolidated_token_counts = Counter([token for tokens in df['tokens'] for token in tokens])\n",
    "token_df = pd.DataFrame(list(consolidated_token_counts.items()), columns=['token', 'count'])\n",
    "token_df = token_df.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Token Consolidation Results:\")\n",
    "for group in grouped_tokens:\n",
    "    most_common = max(group, key=lambda t: token_counts[t])\n",
    "    print(f\"\\nGroup consolidated to '{most_common}' ({consolidated_token_counts[most_common]}):\")\n",
    "    for token in sorted(group):\n",
    "        if token != most_common:\n",
    "            print(f\"  - {token} ({token_counts[token]}) -> {most_common}\")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTotal number of unique original tokens: {len(unique_tokens)}\")\n",
    "print(f\"Total number of unique consolidated tokens: {len(set(token_mapping.values()))}\")\n",
    "print(f\"Number of tokens consolidated: {len(token_mapping) - len(set(token_mapping.values()))}\")\n",
    "\n",
    "# Sample of original vs consolidated tokens\n",
    "print(\"\\nSample of original vs consolidated tokens:\")\n",
    "for _, row in df.head().iterrows():\n",
    "    print(f\"\\nOriginal: {row['original_tokens']}\")\n",
    "    print(f\"Consolidated: {row['tokens']}\")\n",
    "\n",
    "print(\"\\nNote: The original tokens are now stored in the 'original_tokens' column.\")\n",
    "print(\"The 'tokens' column and 'token_df' now contain the consolidated tokens.\")\n",
    "print(\"All downstream processes will now use the consolidated tokens automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "def get_frequent_tokens(df: pd.DataFrame, min_frequency: int = 2) -> set:\n",
    "    all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    return {token for token, count in token_counts.items() if count >= min_frequency}\n",
    "\n",
    "def initialize_entries(df: pd.DataFrame, places: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert DataFrame rows to Entry format with normalization and 'alien' handling,\n",
    "    considering only tokens with frequency >= 9.\n",
    "    Place tokens are replaced with '[cidade]' and always at the end of the entry.\n",
    "    \"\"\"\n",
    "    frequent_tokens = get_frequent_tokens(df)\n",
    "    max_tokens = df['tokens'].apply(lambda x: len([token for token in x if token in frequent_tokens])).max()\n",
    "    \n",
    "    entries = []\n",
    "    for index, row in df.iterrows():\n",
    "        filtered_tokens = []\n",
    "        cidade_token = None\n",
    "        for token in row['tokens']:\n",
    "            if token in places:\n",
    "                cidade_token = '[cidade]'\n",
    "            elif token in frequent_tokens:\n",
    "                filtered_tokens.append(token)\n",
    "        \n",
    "        # Add '[cidade]' token at the end if it exists\n",
    "        if cidade_token:\n",
    "            filtered_tokens.append(cidade_token)\n",
    "        \n",
    "        # Normalize the number of tokens\n",
    "        normalized_tokens = filtered_tokens + ['null'] * (max_tokens - len(filtered_tokens))\n",
    "        \n",
    "        # Check if all tokens are null, replace first with 'alien' if so\n",
    "        if all(token == 'null' for token in normalized_tokens):\n",
    "            normalized_tokens[0] = 'alien'\n",
    "        \n",
    "        entry = np.array([index, np.array(normalized_tokens, dtype=object)], dtype=object)\n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "class ClassificationStructure:\n",
    "    def __init__(self):\n",
    "        self.classifications = {}  # Dictionary to store classifications and their entry indices\n",
    "        self.entries = []  # List to store all entries\n",
    "        self.entry_classifications = {}  # Dictionary to store entry indices and their classifications\n",
    "        self.weights = {}  # Dictionary to store weights of classifications\n",
    "\n",
    "    def add_entry(self, entry: np.ndarray, classifications: List[str]):\n",
    "        entry_index = len(self.entries)\n",
    "        self.entries.append(entry)\n",
    "        self.entry_classifications[entry_index] = set()\n",
    "        \n",
    "        for classification in classifications:\n",
    "            if classification and classification != 'null':\n",
    "                if classification not in self.classifications:\n",
    "                    self.classifications[classification] = set()\n",
    "                    self.weights[classification] = 0\n",
    "                \n",
    "                self.classifications[classification].add(entry_index)\n",
    "                self.entry_classifications[entry_index].add(classification)\n",
    "                self.weights[classification] += 1\n",
    "\n",
    "    def remove_entry(self, entry_index: int):\n",
    "        if entry_index in self.entry_classifications:\n",
    "            for classification in self.entry_classifications[entry_index]:\n",
    "                self.classifications[classification].remove(entry_index)\n",
    "                self.weights[classification] -= 1\n",
    "                if len(self.classifications[classification]) == 0:\n",
    "                    del self.classifications[classification]\n",
    "                    del self.weights[classification]\n",
    "            del self.entry_classifications[entry_index]\n",
    "            self.entries[entry_index] = None\n",
    "\n",
    "    def get_entries_with_classifications(self, classifications: List[str]) -> List[np.ndarray]:\n",
    "        if not classifications:\n",
    "            return []\n",
    "        valid_sets = [self.classifications[c] for c in classifications if c in self.classifications]\n",
    "        if not valid_sets:\n",
    "            return []\n",
    "        entry_indices = set.intersection(*valid_sets)\n",
    "        return [self.entries[i] for i in entry_indices if self.entries[i] is not None]\n",
    "\n",
    "    def get_classifications_for_entry(self, entry_index: int) -> set:\n",
    "        return self.entry_classifications.get(entry_index, set())\n",
    "\n",
    "    def get_weight(self, classification: str) -> int:\n",
    "        return self.weights.get(classification, 0)\n",
    "\n",
    "    def get_overlaps(self) -> dict:\n",
    "        overlaps = {}\n",
    "        for entry_index, classifications in self.entry_classifications.items():\n",
    "            if len(classifications) > 1:\n",
    "                overlap_key = frozenset(classifications)\n",
    "                if overlap_key not in overlaps:\n",
    "                    overlaps[overlap_key] = set()\n",
    "                overlaps[overlap_key].add(entry_index)\n",
    "        return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming df is your original DataFrame)\n",
    "cs = ClassificationStructure()\n",
    "\n",
    "# Create Entry representations\n",
    "entries = initialize_entries(df, places)\n",
    "\n",
    "print(f\"Total number of entries: {len(entries)}\")\n",
    "print(\"Sample entry:\", entries[0])\n",
    "\n",
    "# Populate ClassificationStructure\n",
    "for entry in entries:\n",
    "    index, tokens = entry\n",
    "    cs.add_entry(entry, tokens)\n",
    "\n",
    "# Example queries\n",
    "print(\"\\nEntries with 'vara_alpha' (if exists):\", cs.get_entries_with_classifications(['vara']))\n",
    "print(\"\\nClassifications for entry 0:\", cs.get_classifications_for_entry(0))\n",
    "print(\"\\nWeight of 'juizado_beta' (if exists):\", cs.get_weight('juizado_beta'))\n",
    "print(\"\\nEntries with 'sao paulo' (if exists):\", cs.get_entries_with_classifications(['sao paulo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "def hierarchical_sort(entries: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    def sort_level(level_entries: List[np.ndarray], token_index: int) -> List[np.ndarray]:\n",
    "        if not level_entries or token_index >= len(level_entries[0][1]):\n",
    "            return level_entries\n",
    "\n",
    "        # Group entries by token at the current level\n",
    "        groups = defaultdict(list)\n",
    "        for entry in level_entries:\n",
    "            token = entry[1][token_index]\n",
    "            groups[token].append(entry)\n",
    "\n",
    "        # Sort groups by frequency, then alphabetically\n",
    "        sorted_groups = sorted(groups.items(), \n",
    "                               key=lambda x: (-len(x[1]), x[0] if x[0] != 'null' else 'zzz'))\n",
    "\n",
    "        # Recursively sort each group\n",
    "        sorted_entries = []\n",
    "        for _, group in sorted_groups:\n",
    "            sorted_group = sort_level(group, token_index + 1)\n",
    "            sorted_entries.extend(sorted_group)\n",
    "\n",
    "        return sorted_entries\n",
    "\n",
    "    return sort_level(entries, 0)\n",
    "\n",
    "# Assuming 'entries' is your list of entry arrays\n",
    "sorted_entries = hierarchical_sort(entries)\n",
    "\n",
    "# Print the first few sorted entries to verify\n",
    "for entry in sorted_entries:\n",
    "    print(f\"Index: {entry[0]}, Tokens: {entry[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_relationship_map(entries: List[np.ndarray]) -> pd.DataFrame:\n",
    "    # Collect all unique tokens\n",
    "    all_tokens = set()\n",
    "    for entry in entries:\n",
    "        all_tokens.update([token for token in entry[1] if token != 'null'])\n",
    "    all_tokens = sorted(list(all_tokens))\n",
    "\n",
    "    # Create a dictionary to store token relationships\n",
    "    token_relationships = {token: defaultdict(int) for token in all_tokens}\n",
    "\n",
    "    # Analyze entries and update relationships\n",
    "    for entry in entries:\n",
    "        tokens = [token for token in entry[1] if token != 'null']\n",
    "        for i, token1 in enumerate(tokens):\n",
    "            for token2 in tokens[i+1:]:\n",
    "                token_relationships[token1][token2] += 1\n",
    "                token_relationships[token2][token1] += 1\n",
    "\n",
    "    # Create a DataFrame from the relationships\n",
    "    df = pd.DataFrame(index=all_tokens, columns=all_tokens)\n",
    "    \n",
    "    for token1 in all_tokens:\n",
    "        total = sum(token_relationships[token1].values())\n",
    "        for token2 in all_tokens:\n",
    "            if token1 == token2:\n",
    "                df.at[token1, token2] = 1.0  # Diagonal\n",
    "            elif token2 in token_relationships[token1]:\n",
    "                count = token_relationships[token1][token2]\n",
    "                df.at[token1, token2] = count / total\n",
    "            else:\n",
    "                df.at[token1, token2] = 0.0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "import colorspacious\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial import ConvexHull\n",
    "import colorsys\n",
    "\n",
    "def create_token_relationship_map(entries: List[np.ndarray]) -> pd.DataFrame:\n",
    "    # Collect all unique tokens\n",
    "    all_tokens = set()\n",
    "    for entry in entries:\n",
    "        all_tokens.update([token for token in entry[1] if token != 'null'])\n",
    "    all_tokens = sorted(list(all_tokens))\n",
    "\n",
    "    # Create a dictionary to store token relationships\n",
    "    token_relationships = {token: defaultdict(int) for token in all_tokens}\n",
    "\n",
    "    # Analyze entries and update relationships\n",
    "    for entry in entries:\n",
    "        tokens = [token for token in entry[1] if token != 'null']\n",
    "        for i, token1 in enumerate(tokens):\n",
    "            for token2 in tokens[i+1:]:\n",
    "                token_relationships[token1][token2] += 1\n",
    "                token_relationships[token2][token1] += 1\n",
    "\n",
    "    # Create a DataFrame from the relationships\n",
    "    df = pd.DataFrame(index=all_tokens, columns=all_tokens)\n",
    "    \n",
    "    for token1 in all_tokens:\n",
    "        total = sum(token_relationships[token1].values())\n",
    "        for token2 in all_tokens:\n",
    "            if token1 == token2:\n",
    "                df.at[token1, token2] = 1.0  # Diagonal\n",
    "            elif token2 in token_relationships[token1]:\n",
    "                count = token_relationships[token1][token2]\n",
    "                df.at[token1, token2] = count / total\n",
    "            else:\n",
    "                df.at[token1, token2] = 0.0\n",
    "\n",
    "    return df\n",
    "\n",
    "# Color conversion functions\n",
    "def lab_to_rgb(lab_color):\n",
    "    rgb = colorspacious.cspace_convert(lab_color, \"CAM02-UCS\", \"sRGB1\")\n",
    "    return np.clip(rgb, 0, 1)  # Ensure RGB values are between 0 and 1\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb_color):\n",
    "    return '#{:02x}{:02x}{:02x}'.format(int(rgb_color[0]*255), int(rgb_color[1]*255), int(rgb_color[2]*255))\n",
    "\n",
    "def rgb_to_hsv(rgb_color):\n",
    "    return colorsys.rgb_to_hsv(*rgb_color)\n",
    "\n",
    "def hsv_to_rgb(hsv_color):\n",
    "    return colorsys.hsv_to_rgb(*hsv_color)\n",
    "\n",
    "def normalize_hsv_colors(hsv_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]:\n",
    "    h_values, s_values, v_values = zip(*hsv_colors)\n",
    "    \n",
    "    # Find the maximum S and V values\n",
    "    s_max, v_max = max(s_values), max(v_values)\n",
    "    s_min, v_min = min(s_values), min(v_values)\n",
    "    \n",
    "    # Normalize S and V values, but maintain some spread\n",
    "    normalized_colors = [\n",
    "        (h, \n",
    "         0.2 + 0.8 * (s - s_min) / (s_max - s_min) if s_max > s_min else s, \n",
    "         0.2 + 0.8 * (v - v_min) / (v_max - v_min) if v_max > v_min else v)\n",
    "        for h, s, v in hsv_colors\n",
    "    ]\n",
    "    \n",
    "    return normalized_colors\n",
    "\n",
    "def reduce_dimensions(relationship_matrix: np.ndarray) -> np.ndarray:\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    return tsne.fit_transform(relationship_matrix)\n",
    "\n",
    "def cluster_tokens(reduced_data: np.ndarray, n_clusters: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(reduced_data)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    return clusters, cluster_centers\n",
    "\n",
    "def calculate_cluster_proximities(reduced_data: np.ndarray, cluster_centers: np.ndarray) -> np.ndarray:\n",
    "    distances = cdist(reduced_data, cluster_centers)\n",
    "    max_distance = np.max(distances)\n",
    "    return 1 - (distances / max_distance)\n",
    "\n",
    "\n",
    "def calculate_saturation_factor(distances, cluster_labels, i):\n",
    "    assigned_cluster = cluster_labels[i]\n",
    "    assigned_distance = distances[i, assigned_cluster]\n",
    "    \n",
    "    # Find the distance to the next closest cluster\n",
    "    other_distances = [dist for j, dist in enumerate(distances[i]) if j != assigned_cluster]\n",
    "    next_closest_distance = min(other_distances)\n",
    "    \n",
    "    # Calculate how \"decided\" this token's cluster assignment is\n",
    "    decisiveness = (next_closest_distance - assigned_distance) / next_closest_distance\n",
    "    \n",
    "    # Adjust saturation to highlight both cluster centers and boundary regions\n",
    "    if decisiveness > 0.5:\n",
    "        # For tokens firmly in a cluster, reduce saturation for tokens far from center\n",
    "        return 1.0 - (assigned_distance / np.max(distances[:, assigned_cluster])) * 0.5\n",
    "    else:\n",
    "        # For boundary tokens, increase saturation\n",
    "        return 1.0 + (0.5 - decisiveness) * 0.5\n",
    "\n",
    "def assign_token_colors(entries: List[np.ndarray], n_clusters: int = 4) -> Dict[str, Tuple[float, float, float]]:\n",
    "    relationship_matrix = create_token_relationship_map(entries)\n",
    "    reduced_data = reduce_dimensions(relationship_matrix)\n",
    "    \n",
    "    # Perform clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Define base colors in CIELAB space (as float64)\n",
    "    base_colors = np.array([\n",
    "        [90.0, -150.0, 0.0],    # Green\n",
    "        [90.0, 150.0, 0.0],     # Red\n",
    "        [90.0, 0.0, -150.0],    # Blue\n",
    "        [90.0, 0.0, 150.0]      # Yellow\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    token_colors = {}\n",
    "    tokens = relationship_matrix.index.tolist()\n",
    "    \n",
    "    # Calculate distances to cluster centers\n",
    "    distances = distance_matrix(reduced_data, cluster_centers)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Initialize color as float64\n",
    "        color = np.zeros(3, dtype=np.float64)\n",
    "        \n",
    "        for j in range(n_clusters):\n",
    "            # Calculate influence, ensuring float64 output\n",
    "            influence = 1.0 / (distances[i, j] ** 2)\n",
    "            color += base_colors[j] * influence\n",
    "        \n",
    "        # Normalize color\n",
    "        norm = np.linalg.norm(color)\n",
    "        if norm != 0:\n",
    "            color /= norm\n",
    "        \n",
    "        # Calculate and apply the new saturation factor\n",
    "        saturation_factor = calculate_saturation_factor(distances, cluster_labels, i)\n",
    "        color[1:] *= saturation_factor  # Adjust a* and b* components\n",
    "        \n",
    "        token_colors[token] = tuple(color)\n",
    "    \n",
    "    # Assign white color to 'null' token\n",
    "    token_colors['null'] = (100.0, 0.0, 0.0)  # White in CIELAB\n",
    "    \n",
    "    return token_colors, reduced_data, cluster_labels\n",
    "\n",
    "def blend_entry_colors(entry: np.ndarray, token_colors: Dict[str, Tuple[float, float, float]]) -> Tuple[float, float, float]:\n",
    "    colors = [token_colors.get(token, token_colors['null']) for token in entry[1]]\n",
    "    \n",
    "    if not colors or all(color == token_colors['null'] for color in colors):\n",
    "        return (100, 0, 0)  # White for empty entries or entries with only null tokens\n",
    "    \n",
    "    # Filter out null colors for blending\n",
    "    non_null_colors = [color for color in colors if color != token_colors['null']]\n",
    "    \n",
    "    if not non_null_colors:\n",
    "        return (100, 0, 0)  # White if all tokens were null\n",
    "    \n",
    "    # Calculate weights based on position\n",
    "    num_tokens = len(non_null_colors)\n",
    "    weights = np.linspace(1, 0.5, num_tokens)  # Linear decrease from 1 to 0.5\n",
    "    weights = weights / np.sum(weights)  # Normalize weights to sum to 1\n",
    "    \n",
    "    # Calculate the weighted average for each component\n",
    "    l_blend = np.average([c[0] for c in non_null_colors], weights=weights)\n",
    "    a_blend = np.average([c[1] for c in non_null_colors], weights=weights)\n",
    "    b_blend = np.average([c[2] for c in non_null_colors], weights=weights)\n",
    "    \n",
    "    # Darken based on the number of non-null tokens\n",
    "    darkness_factor = min(num_tokens * 0.05, 0.3)  # Max 30% darkness\n",
    "    l_blend *= (1 - darkness_factor)\n",
    "    \n",
    "    # Increase saturation\n",
    "    saturation_factor = 1 + (num_tokens * 0.05)  # Increase saturation based on non-null tokens\n",
    "    a_blend *= saturation_factor\n",
    "    b_blend *= saturation_factor\n",
    "    \n",
    "    return (l_blend, a_blend, b_blend)\n",
    "\n",
    "def visualize_token_clusters(reduced_data: np.ndarray, clusters: np.ndarray, tokens: List[str]):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    # Ensure the number of annotations is within the bounds of the reduced_data array\n",
    "    num_points = min(len(reduced_data), len(tokens))\n",
    "    \n",
    "    for i in range(0, num_points, max(1, num_points // 10)):  # Annotate about 10 points\n",
    "        plt.annotate(tokens[i], (reduced_data[i, 0], reduced_data[i, 1]))\n",
    "    \n",
    "    plt.title('Token Clusters in 2D Space')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_token_colors(reduced_data: np.ndarray, token_colors: Dict[str, Tuple[float, float, float]], cluster_labels: np.ndarray):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Convert LAB colors to RGB for plotting\n",
    "    rgb_colors = [lab_to_rgb(np.array(color, dtype=np.float64)) for token, color in token_colors.items() if token != 'null']\n",
    "    valid_data = reduced_data[[i for i, (token, _) in enumerate(token_colors.items()) if token != 'null']]\n",
    "    valid_labels = cluster_labels[[i for i, (token, _) in enumerate(token_colors.items()) if token != 'null']]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = plt.scatter(valid_data[:, 0], valid_data[:, 1], c=rgb_colors, s=50, alpha=0.7)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Token Color Distribution in Clusters', fontsize=16)\n",
    "    plt.xlabel('Dimension 1', fontsize=14)\n",
    "    plt.ylabel('Dimension 2', fontsize=14)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add a light grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Adjust tick label size\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, label='Color', aspect=30)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Add cluster boundaries\n",
    "    for cluster in range(4):\n",
    "        cluster_points = valid_data[valid_labels == cluster]\n",
    "        if len(cluster_points) > 2:  # Need at least 3 points to create a convex hull\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            for simplex in hull.simplices:\n",
    "                plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1], 'k-', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_color_distribution(token_colors: Dict[str, Tuple[float, float, float]]):\n",
    "    colors_lab = np.array(list(token_colors.values()))\n",
    "    colors_rgb = np.array([lab_to_rgb(color) for color in colors_lab])\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    scatter1 = ax1.scatter(colors_lab[:, 1], colors_lab[:, 2], c=colors_rgb)\n",
    "    ax1.set_xlabel('a* (Green-Red)')\n",
    "    ax1.set_ylabel('b* (Blue-Yellow)')\n",
    "    ax1.set_title('Token Color Distribution in a*b* plane')\n",
    "    ax1.set_xlim(-128, 128)\n",
    "    ax1.set_ylim(-128, 128)\n",
    "    fig.colorbar(scatter1, ax=ax1, label='L* value')\n",
    "    \n",
    "    scatter2 = ax2.scatter(colors_lab[:, 1], colors_lab[:, 0], c=colors_rgb)\n",
    "    ax2.set_xlabel('a* (Green-Red)')\n",
    "    ax2.set_ylabel('L* (Lightness)')\n",
    "    ax2.set_title('Token Color Distribution in L*a* plane')\n",
    "    ax2.set_xlim(-128, 128)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    fig.colorbar(scatter2, ax=ax2, label='b* value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_entry_colors(entries: List[np.ndarray], token_colors: Dict[str, Tuple[float, float, float]]):\n",
    "    # Blend colors for each entry\n",
    "    entry_colors_lab = [blend_entry_colors(entry, token_colors) for entry in entries]\n",
    "    entry_colors_rgb = [lab_to_rgb(color) for color in entry_colors_lab]\n",
    "    \n",
    "    # Reduce dimensions of entries for visualization\n",
    "    entry_matrix = np.array([np.bincount(entry[1] != 'null', minlength=len(token_colors)) for entry in entries])\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_entries = tsne.fit_transform(entry_matrix)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(reduced_entries[:, 0], reduced_entries[:, 1], c=entry_colors_rgb)\n",
    "    \n",
    "    plt.title('Entry Colors in 2D Space')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.colorbar(scatter, label='Color')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_color_mosaic(sorted_entries: List[Tuple[np.ndarray, Tuple[float, float, float], str]], output_file: str = 'color_mosaic.png'):\n",
    "    n = len(sorted_entries)\n",
    "    grid_size = int(np.ceil(np.sqrt(n)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    for i, (entry, _, hex_color) in enumerate(sorted_entries):\n",
    "        row = i // grid_size\n",
    "        col = i % grid_size\n",
    "        ax.add_patch(plt.Rectangle((col, grid_size - row - 1), 1, 1, facecolor=hex_color))\n",
    "    \n",
    "    ax.set_xlim(0, grid_size)\n",
    "    ax.set_ylim(0, grid_size)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Color mosaic saved as {output_file}\")\n",
    "\n",
    "def sort_entries_by_color(entries: List[np.ndarray], token_colors: Dict[str, Tuple[float, float, float]]) -> List[Tuple[np.ndarray, Tuple[float, float, float], str]]:\n",
    "    entry_colors = []\n",
    "    for entry in entries:\n",
    "        lab_color = blend_entry_colors(entry, token_colors)\n",
    "        rgb_color = lab_to_rgb(lab_color)\n",
    "        hsv_color = rgb_to_hsv(rgb_color)\n",
    "        entry_colors.append((entry, rgb_color, hsv_color))\n",
    "    \n",
    "    # Normalize HSV colors\n",
    "    _, _, hsv_colors = zip(*entry_colors)\n",
    "    normalized_hsv_colors = normalize_hsv_colors(hsv_colors)\n",
    "    \n",
    "    # Create new list with normalized colors\n",
    "    normalized_entry_colors = []\n",
    "    for (entry, _, _), normalized_hsv in zip(entry_colors, normalized_hsv_colors):\n",
    "        if any(np.isnan(normalized_hsv)):\n",
    "            print(f\"Found NaN in HSV values for entry: {entry}\")\n",
    "            continue  # Skip this entry if NaN is found\n",
    "        \n",
    "        normalized_rgb = hsv_to_rgb(normalized_hsv)\n",
    "        hex_color = rgb_to_hex(normalized_rgb)\n",
    "        normalized_entry_colors.append((entry, normalized_rgb, hex_color, normalized_hsv))\n",
    "    \n",
    "    # Sort by hue, then saturation, then value\n",
    "    sorted_entries = sorted(normalized_entry_colors, key=lambda x: x[3])\n",
    "    \n",
    "    # Remove HSV color from the returned list\n",
    "    return [(entry, rgb, hex) for entry, rgb, hex, _ in sorted_entries]\n",
    "\n",
    "\n",
    "# Assuming 'entries' is your list of processed entry arrays\n",
    "token_colors, reduced_data, cluster_labels = assign_token_colors(entries)\n",
    "\n",
    "# Sort entries by color and create mosaic\n",
    "sorted_entries = sort_entries_by_color(entries, token_colors)\n",
    "\n",
    "# Add a color distribution analysis\n",
    "print(\"\\nColor Distribution Analysis:\")\n",
    "color_counts = defaultdict(int)\n",
    "for _, rgb_color, _ in sorted_entries:\n",
    "    hsv_color = rgb_to_hsv(rgb_color)\n",
    "    hue_category = int(hsv_color[0] * 4)  # Divide hue into 4 categories\n",
    "    color_counts[hue_category] += 1\n",
    "\n",
    "for category, count in color_counts.items():\n",
    "    print(f\"Color category {category}: {count} entries\")\n",
    "\n",
    "# Visualizations\n",
    "# Call the visualization functions\n",
    "# Check for any mismatches\n",
    "mismatched_tokens = set(tokens) - set(token_colors.keys())\n",
    "if mismatched_tokens:\n",
    "    print(f\"Tokens in 'tokens' but not in 'token_colors': {mismatched_tokens}\")\n",
    "\n",
    "# Visualize only if data is consistent\n",
    "visualize_token_colors(reduced_data, token_colors, cluster_labels)\n",
    "visualize_entry_colors(entries, token_colors)\n",
    "visualize_color_mosaic(sorted_entries, 'normalized_entry_color_mosaic_prototype.png')\n",
    "\n",
    "# Print example results\n",
    "print(\"\\nExample Sorted Entries (first 10):\")\n",
    "for entry, rgb_color, hex_color in sorted_entries[:10]:\n",
    "    hsv_color = rgb_to_hsv(rgb_color)\n",
    "    print(f\"Entry: {entry[1]}, RGB: {rgb_color}, Hex: {hex_color}, HSV: {hsv_color}\")\n",
    "\n",
    "# Print example Token Colors\n",
    "print(\"\\nExample Token Colors (in CIELAB space):\")\n",
    "for token, color in list(token_colors.items())[:10]:\n",
    "    print(f\"{token}: LAB{color}\")\n",
    "\n",
    "print(\"\\nExample Token Colors (in sRGB space):\")\n",
    "for token, lab_color in list(token_colors.items())[:10]:\n",
    "    rgb_color = lab_to_rgb(lab_color)\n",
    "    print(f\"{token}: RGB{rgb_color}\")\n",
    "\n",
    "print(\"\\nExample Sorted Entries (first 10):\")\n",
    "for entry, rgb_color, hex_color in sorted_entries[:10]:\n",
    "    print(f\"Entry: {entry[1]}, RGB: {rgb_color}, Hex: {hex_color}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
