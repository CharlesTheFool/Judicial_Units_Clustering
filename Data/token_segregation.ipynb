{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique names: 2934\n",
      "Number of unique places: 2279\n",
      "\n",
      "First 10 names:\n",
      "['abaetetuba', 'abaetetuba', 'abel', 'abelardo', 'abelha', 'abensur', 'abraham', 'abreu', 'acacio', 'acarau']\n",
      "\n",
      "First 10 places:\n",
      "['abadia/go', 'abadiania', 'abaete', 'abare', 'aberto', 'abre campo/mg', 'acailandia', 'acailandia/ma', 'acajutiba', 'acara']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def tokenize_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().lower()\n",
    "    # Remove accents\n",
    "    content = remove_accents(content)\n",
    "    # Split by comma or newline, then strip whitespace\n",
    "    tokens = [token.strip() for token in re.split(r'[,\\n]', content) if token.strip()]\n",
    "    return tokens\n",
    "\n",
    "# Read and tokenize both files\n",
    "names = tokenize_file('name_segregated_tokens.txt')\n",
    "places = tokenize_file('place_segregated_tokens.txt')\n",
    "\n",
    "# Remove duplicates\n",
    "names = list(set(names))\n",
    "places = list(set(places))\n",
    "\n",
    "# Move common tokens from places to names\n",
    "common_tokens = set(names) & set(places)\n",
    "names.extend(common_tokens)\n",
    "places = [place for place in places if place not in common_tokens]\n",
    "\n",
    "# Sort the lists\n",
    "names.sort()\n",
    "places.sort()\n",
    "\n",
    "print(f\"Number of unique names: {len(names)}\")\n",
    "print(f\"Number of unique places: {len(places)}\")\n",
    "\n",
    "# Optional: Print the first few elements of each list to verify\n",
    "print(\"\\nFirst 10 names:\")\n",
    "print(names[:10])\n",
    "print(\"\\nFirst 10 places:\")\n",
    "print(places[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with frequency 2 or more before filtering: 2767\n",
      "Number of tokens after filtering out places and names: 2767\n",
      "Filtered tokens have been written to 'tokens_processed.txt'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('unidades.csv')\n",
    "\n",
    "# Display the first few rows to verify data\n",
    "df.head()\n",
    "\n",
    "# Define a function to remove accents\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# Define synonyms\n",
    "synonyms = {\n",
    "    'gab': 'gabinete',\n",
    "    'gab.': 'gabinete',\n",
    "    'presidencia': 'presidencia',\n",
    "    'v': 'vara',\n",
    "    'var': 'vara',\n",
    "    'vio': 'violencia',\n",
    "    'c': 'circunscricao',\n",
    "    'juiza': 'juiz',\n",
    "    'substituta': 'substituto',\n",
    "    'dra': 'dr',\n",
    "    'faz': 'fazenda',\n",
    "    'fam': 'familia',\n",
    "    'exma': 'exmo',\n",
    "    'reg': 'registros',\n",
    "    'pub': 'publico',\n",
    "    'publ': 'publico',\n",
    "    'publica': 'publico',\n",
    "    'juv': 'juventude',\n",
    "    'inf': 'infancia',\n",
    "    'crim': 'criminal',\n",
    "    'DEECRIM': 'criminal',\n",
    "    'adj': 'adjunto',\n",
    "    'cons': 'consumo',\n",
    "    'jef': 'federal',\n",
    "    'jud': 'judiciario',\n",
    "    'desembargadora': 'desembargador',\n",
    "    'des': 'desembargador',\n",
    "    'desa': 'desembargador',\n",
    "    'desemb': 'desembargador',\n",
    "    'j': 'juizado',\n",
    "    'jui': 'juizado',\n",
    "    'civ': 'civel',\n",
    "    'civeis': 'civel',\n",
    "    'civil': 'civel',\n",
    "    'esp': 'especial',\n",
    "    'especiais': 'especial',\n",
    "    'educativa': 'educacional',\n",
    "    'contadoria/tesouraria': 'contadoria',\n",
    "    'c/mulher': 'mulher',\n",
    "    'calculos': 'calculo',\n",
    "    'calc': 'calculo',\n",
    "    'mulh': 'mulher',\n",
    "    'adm': 'administracao',\n",
    "    'amb': 'ambiental',\n",
    "    'acomp': 'acompanhamento',\n",
    "    'aten': 'atencao',\n",
    "    'atend': 'atendimento',\n",
    "    'aux': 'auxiliar',\n",
    "    'aval': 'avaliacao',\n",
    "    'compet': 'competencia',\n",
    "    'conf': 'conflito',\n",
    "    'confl': 'conflito',\n",
    "    'coord': 'coordenacao',\n",
    "    'cump': 'cumprimento',\n",
    "    'def': 'defensoria',\n",
    "    'dep': 'departamento',\n",
    "    'dist': 'distribuicao',\n",
    "    'distr': 'distribuicao',\n",
    "    'exec': 'execucao',\n",
    "    'fisc': 'fiscal',\n",
    "    'gest': 'gestao',\n",
    "    'inform': 'informacao',\n",
    "    'inq': 'inquerito',\n",
    "    'jurid': 'juridico',\n",
    "    'med': 'mediacao',\n",
    "    'mun': 'municipal',\n",
    "    'munic': 'municipal',\n",
    "    'org': 'organizacao',\n",
    "    'pres': 'presidencia',\n",
    "    'proc': 'processo',\n",
    "    'prog': 'programa',\n",
    "    'proj': 'projeto',\n",
    "    'prot': 'protocolo',\n",
    "    'rec': 'recurso',\n",
    "    'rel': 'relator',\n",
    "    'sec': 'secretaria',\n",
    "    'serv': 'servico',\n",
    "    'sist': 'sistema',\n",
    "    'tec': 'tecnico',\n",
    "    'trib': 'tribunal',\n",
    "    'unid': 'unidade',\n",
    "    'fg' : 'fig',\n",
    "    'gm' : 'gmf',\n",
    "    '[microrregiao' : 'microrregiao',\n",
    "\n",
    "\n",
    "    # Add more synonyms as needed\n",
    "}\n",
    "\n",
    "# Define multi-token replacements\n",
    "multi_token_replacements = {\n",
    "    'vt': ['vara', 'trabalho'],\n",
    "    'cejusc' :['centro','judicial','solucao','conflitos','cidadania'],\n",
    "    'tr': ['turma', 'recursal'],\n",
    "    'jit': ['juizado', 'especial','civel'],\n",
    "    'gades': ['gabinete', 'desembargador'],\n",
    "    'jesp': ['juizado', 'especial','criminal'],\n",
    "    'jec': ['juizado', 'especial','civel'],\n",
    "    'saf': ['servico', 'anexo','fazendas'],\n",
    "    # Add more multi-token replacements as needed\n",
    "}\n",
    "\n",
    "# Function to replace synonyms and multi-token replacements\n",
    "def replace_synonyms_and_multi_tokens(token):\n",
    "    if token in multi_token_replacements:\n",
    "        return multi_token_replacements[token]\n",
    "    else:\n",
    "        return [synonyms.get(token, token)]\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_name(name, additional_stopwords=None):\n",
    "    name = remove_accents(name).lower()\n",
    "    \n",
    "    # REPLACE 'CJ' OR 'C J' WITH 'CIRCUNSCRICAO JUDICIAL'\n",
    "    name = re.sub(r'\\bc\\s*j\\b', 'circunscricao', name)\n",
    "    \n",
    "    # REMOVE NUMBERS AND NUMBER-LETTER COMBINATIONS, BUT KEEP THE PRECEDING WORD\n",
    "    name = re.sub(r'\\b(\\d+\\w*)\\b', '', name)\n",
    "    \n",
    "    # COMBINE 'GRAU' WITH THE PRECEDING WORD\n",
    "    name = re.sub(r'(\\b\\w+\\b)\\s+grau', r'\\1_grau', name)\n",
    "    \n",
    "    # COMBINE 'VICE' WITH THE FOLLOWING WORD (WITH SPACE OR HYPHEN) INTO A SINGLE TOKEN\n",
    "    name = re.sub(r'\\b(vice)[-\\s]+(\\w+)', r'\\1_\\2', name)\n",
    "\n",
    "    \n",
    "    tokens = re.split(r'\\s|,|\\.|\\(|\\)|-', name)\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]\n",
    "\n",
    "    stopwords = ['de', 'da', 'do', 'das', 'dos', \n",
    "                 'e', 'a', 'o', 'i', 'u', 'b', 'as', 'ao',\n",
    "                 '\"', 'em', 'des', 'com', 'n', 'g', 'ap', 'sr', 'sra','/', '\\\\' ,'?', '\\'', '\\\\\\'', '\\\"gabinete', 'ou',\n",
    "                'hora', 'solteira', 'villa','zz', '°', '¿',\n",
    "                'i','ii','iii','iv','v','vi','vii','viii','ix','x',\n",
    "                'xi','xii','xiii','xiv','xv','xvi','xvii','xviii','xix','xx',\n",
    "                'xxi','xxii','xxiii','xxiv','xxv','xxvi','xxvii','xxviii','xxix','xxx',\n",
    "                'xxxi','xxxii','xxxiii','xxxiv','xxxv','xxxvi','xxxvii','xxxviii','xxxix','xl',\n",
    "                'xli','xlii','xliii','xliv','xlv','xlvi','xlvii','xlviii','xlix','l',\n",
    "                'li','lii','liii','liv','lv','lvi','lvii','lviii','lix','lx',\n",
    "                'lxi','lxii','lxiii','lxiv','lxv','lxvi','lxvii','lxviii','lxix','lxx',\n",
    "                'lxxi','lxxii','lxxiii','lxxiv','lxxv','lxxvi','lxxvii','lxxviii','lxxix','lxxx',\n",
    "                'lxxxi','lxxxii','lxxxiii','lxxxiv','lxxxv','lxxxvi','lxxxvii','lxxxviii','lxxxix','xc',\n",
    "                'xci','xcii','xciii','xciv','xcv','xcvi','xcvii','xcviii','xcix','c','sanclerlandia','goianapolis','?',':','ci','cii','varao', '\\\"\"']\n",
    "\n",
    "    if additional_stopwords:\n",
    "        stopwords.extend(additional_stopwords)\n",
    "    \n",
    "\n",
    "    def combine_words(name, stopwords):\n",
    "        # Define the words to be combined\n",
    "        words_to_combine = [\n",
    "            'sao', 'santa', 'santo', 'nova', 'novo', 'bom', 'boa', \n",
    "            'alto', 'alta', 'baixo', 'baixa', 'porto', 'campos', \n",
    "            'rio', 'foz', 'barra', 'passa', 'entre'\n",
    "        ]\n",
    "\n",
    "        # Function to replace word and its following non-stopword\n",
    "        def replace_word(match):\n",
    "            word1 = match.group(1)\n",
    "            word2 = match.group(2)\n",
    "            if word2.lower() not in stopwords:\n",
    "                return f'{word1}_{word2}'\n",
    "            return f'{word1} {word2}'\n",
    "\n",
    "        # Create a combined regex pattern for all words to be combined\n",
    "        pattern = r'\\b(' + '|'.join(words_to_combine) + r')[\\s-]+(\\w+)'\n",
    "\n",
    "        # Apply the replacement\n",
    "        name = re.sub(pattern, replace_word, name)\n",
    "\n",
    "        return name\n",
    "    \n",
    "    name = combine_words(name, stopwords)\n",
    "\n",
    "    stopwords.extend([name.lower() for name in names])\n",
    "\n",
    "    if additional_stopwords:\n",
    "        stopwords.extend(additional_stopwords)\n",
    "\n",
    "    tokens = re.split(r'\\s|,|\\.|\\(|\\)|-', name)\n",
    "    tokens = [token.strip() for token in tokens if token.strip() and token not in stopwords]\n",
    "\n",
    "    \n",
    "    # PROCESS EACH TOKEN, APPLYING SYNONYMS AND MULTI-TOKEN REPLACEMENTS\n",
    "    processed_tokens = []\n",
    "    skip_next = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        if token and token not in stopwords:\n",
    "            # HANDLE CASE WHERE 'C' IS FOLLOWED BY 'J'\n",
    "            if token == 'c' and i + 1 < len(tokens) and tokens[i + 1] == 'j':\n",
    "                processed_tokens.append('circunscricao')\n",
    "                skip_next = True\n",
    "            else:\n",
    "                processed_tokens.extend(replace_synonyms_and_multi_tokens(token))\n",
    "    \n",
    "    # REPLACE UNDERSCORES WITH SPACES IN PRESERVED CONJOINED EXPRESSIONS\n",
    "    processed_tokens = [token.replace('_', ' ') for token in processed_tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# Apply tokenization and store original names\n",
    "df['tokens'] = df['nomeUnidade'].apply(lambda x: tokenize_name(x))\n",
    "df['original_name'] = df['nomeUnidade']\n",
    "\n",
    "# Apply tokenization and store original names\n",
    "df['tokens'] = df['nomeUnidade'].apply(lambda x: tokenize_name(x))\n",
    "df['original_name'] = df['nomeUnidade']\n",
    "\n",
    "# Flatten list of tokens\n",
    "all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "\n",
    "# Calculate frequency of each token\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Filter tokens with frequency 2 or more\n",
    "frequent_tokens = [token for token, count in token_counts.items() if count >= 2]\n",
    "\n",
    "# Combine places and names into a single set for faster lookup\n",
    "exclude_set = []\n",
    "\n",
    "# Filter out tokens that appear in places or names lists\n",
    "filtered_tokens = [token for token in frequent_tokens if token.lower() not in exclude_set]\n",
    "\n",
    "# Sort filtered tokens alphabetically\n",
    "filtered_tokens.sort()\n",
    "\n",
    "# Write filtered tokens to file\n",
    "with open('tokens_processed.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(', '.join(filtered_tokens))\n",
    "\n",
    "print(f\"Number of tokens with frequency 2 or more before filtering: {len(frequent_tokens)}\")\n",
    "print(f\"Number of tokens after filtering out places and names: {len(filtered_tokens)}\")\n",
    "print(\"Filtered tokens have been written to 'tokens_processed.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
