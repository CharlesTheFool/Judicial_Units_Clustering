{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def tokenize_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().lower()\n",
    "    # Remove accents\n",
    "    content = remove_accents(content)\n",
    "    # Split by comma or newline, then strip whitespace\n",
    "    tokens = [token.strip() for token in re.split(r'[,\\n]', content) if token.strip()]\n",
    "    return tokens\n",
    "\n",
    "# Read and tokenize both files\n",
    "names = tokenize_file('name_segregated_tokens.txt')\n",
    "places = tokenize_file('place_segregated_tokens.txt')\n",
    "\n",
    "# Remove duplicates\n",
    "names = list(set(names))\n",
    "places = list(set(places))\n",
    "\n",
    "# Move common tokens from places to names\n",
    "common_tokens = set(names) & set(places)\n",
    "names.extend(common_tokens)\n",
    "places = [place for place in places if place not in common_tokens]\n",
    "\n",
    "# Sort the lists\n",
    "names.sort()\n",
    "places.sort()\n",
    "\n",
    "print(f\"Number of unique names: {len(names)}\")\n",
    "print(f\"Number of unique places: {len(places)}\")\n",
    "\n",
    "# Optional: Print the first few elements of each list to verify\n",
    "print(\"\\nFirst 10 names:\")\n",
    "print(names[:10])\n",
    "print(\"\\nFirst 10 places:\")\n",
    "print(places[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../unidades.csv')\n",
    "\n",
    "# Display the first few rows to verify data\n",
    "df.head()\n",
    "\n",
    "# Define a function to remove accents\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# Define synonyms\n",
    "synonyms = {\n",
    "    'gab': 'gabinete',\n",
    "    'gab.': 'gabinete',\n",
    "    'presidencia': 'presidencia',\n",
    "    'v': 'vara',\n",
    "    'var': 'vara',\n",
    "    'vio': 'violencia',\n",
    "    'c': 'circunscricao',\n",
    "    'juiza': 'juiz',\n",
    "    'substituta': 'substituto',\n",
    "    'dra': 'dr',\n",
    "    'faz': 'fazenda',\n",
    "    'fam': 'familia',\n",
    "    'exma': 'exmo',\n",
    "    'reg': 'registros',\n",
    "    'pub': 'publico',\n",
    "    'publ': 'publico',\n",
    "    'publica': 'publico',\n",
    "    'juv': 'juventude',\n",
    "    'inf': 'infancia',\n",
    "    'crim': 'criminal',\n",
    "    'DEECRIM': 'criminal',\n",
    "    'adj': 'adjunto',\n",
    "    'cons': 'consumo',\n",
    "    'jef': 'federal',\n",
    "    'jud': 'judiciario',\n",
    "    'desembargadora': 'desembargador',\n",
    "    'des': 'desembargador',\n",
    "    'desa': 'desembargador',\n",
    "    'desemb': 'desembargador',\n",
    "    'j': 'juizado',\n",
    "    'jui': 'juizado',\n",
    "    'civ': 'civel',\n",
    "    'civeis': 'civel',\n",
    "    'civil': 'civel',\n",
    "    'esp': 'especial',\n",
    "    'especiais': 'especial',\n",
    "    'educativa': 'educacional',\n",
    "    'contadoria/tesouraria': 'contadoria',\n",
    "    'c/mulher': 'mulher',\n",
    "    'calculos': 'calculo',\n",
    "    'calc': 'calculo',\n",
    "    'mulh': 'mulher',\n",
    "    'adm': 'administracao',\n",
    "    'amb': 'ambiental',\n",
    "    'acomp': 'acompanhamento',\n",
    "    'aten': 'atencao',\n",
    "    'atend': 'atendimento',\n",
    "    'aux': 'auxiliar',\n",
    "    'aval': 'avaliacao',\n",
    "    'compet': 'competencia',\n",
    "    'conf': 'conflito',\n",
    "    'confl': 'conflito',\n",
    "    'coord': 'coordenacao',\n",
    "    'cump': 'cumprimento',\n",
    "    'def': 'defensoria',\n",
    "    'dep': 'departamento',\n",
    "    'dist': 'distribuicao',\n",
    "    'distr': 'distribuicao',\n",
    "    'exec': 'execucao',\n",
    "    'fisc': 'fiscal',\n",
    "    'gest': 'gestao',\n",
    "    'inform': 'informacao',\n",
    "    'inq': 'inquerito',\n",
    "    'jurid': 'juridico',\n",
    "    'med': 'mediacao',\n",
    "    'mun': 'municipal',\n",
    "    'munic': 'municipal',\n",
    "    'org': 'organizacao',\n",
    "    'pres': 'presidencia',\n",
    "    'proc': 'processo',\n",
    "    'prog': 'programa',\n",
    "    'proj': 'projeto',\n",
    "    'prot': 'protocolo',\n",
    "    'rec': 'recurso',\n",
    "    'rel': 'relator',\n",
    "    'sec': 'secretaria',\n",
    "    'serv': 'servico',\n",
    "    'sist': 'sistema',\n",
    "    'tec': 'tecnico',\n",
    "    'trib': 'tribunal',\n",
    "    'unid': 'unidade',\n",
    "    'fg' : 'fig',\n",
    "    'gm' : 'gmf',\n",
    "    '[microrregiao' : 'microrregiao',\n",
    "\n",
    "\n",
    "    # Add more synonyms as needed\n",
    "}\n",
    "\n",
    "# Define multi-token replacements\n",
    "multi_token_replacements = {\n",
    "    'vt': ['vara', 'trabalho'],\n",
    "    'cejusc' :['centro','judicial','solucao','conflitos','cidadania'],\n",
    "    'tr': ['turma', 'recursal'],\n",
    "    'jit': ['juizado', 'especial','civel'],\n",
    "    'gades': ['gabinete', 'desembargador'],\n",
    "    'jesp': ['juizado', 'especial','criminal'],\n",
    "    'jec': ['juizado', 'especial','civel'],\n",
    "    'saf': ['servico', 'anexo','fazendas'],\n",
    "    # Add more multi-token replacements as needed\n",
    "}\n",
    "\n",
    "# Function to replace synonyms and multi-token replacements\n",
    "def replace_synonyms_and_multi_tokens(token):\n",
    "    if token in multi_token_replacements:\n",
    "        return multi_token_replacements[token]\n",
    "    else:\n",
    "        return [synonyms.get(token, token)]\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_name(name, additional_stopwords=None):\n",
    "    name = remove_accents(name).lower()\n",
    "    \n",
    "    # REPLACE 'CJ' OR 'C J' WITH 'CIRCUNSCRICAO JUDICIAL'\n",
    "    name = re.sub(r'\\bc\\s*j\\b', 'circunscricao', name)\n",
    "    \n",
    "    # REMOVE NUMBERS AND NUMBER-LETTER COMBINATIONS, BUT KEEP THE PRECEDING WORD\n",
    "    name = re.sub(r'\\b(\\d+\\w*)\\b', '', name)\n",
    "    \n",
    "    # COMBINE 'GRAU' WITH THE PRECEDING WORD\n",
    "    name = re.sub(r'(\\b\\w+\\b)\\s+grau', r'\\1_grau', name)\n",
    "    \n",
    "    # COMBINE 'VICE' WITH THE FOLLOWING WORD (WITH SPACE OR HYPHEN) INTO A SINGLE TOKEN\n",
    "    name = re.sub(r'\\b(vice)[-\\s]+(\\w+)', r'\\1_\\2', name)\n",
    "\n",
    "    \n",
    "    tokens = re.split(r'\\s|,|\\.|\\(|\\)|-', name)\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]\n",
    "\n",
    "    stopwords = ['de', 'da', 'do', 'das', 'dos', \n",
    "                 'e', 'a', 'o', 'i', 'u', 'b', 'as', 'ao',\n",
    "                 '\"', 'em', 'des', 'com', 'n', 'g', 'ap', 'sr', 'sra','/',\n",
    "                'hora', 'solteira', 'villa','zz', '°', '¿',\n",
    "                'i','ii','iii','iv','v','vi','vii','viii','ix','x',\n",
    "                'xi','xii','xiii','xiv','xv','xvi','xvii','xviii','xix','xx',\n",
    "                'xxi','xxii','xxiii','xxiv','xxv','xxvi','xxvii','xxviii','xxix','xxx',\n",
    "                'xxxi','xxxii','xxxiii','xxxiv','xxxv','xxxvi','xxxvii','xxxviii','xxxix','xl',\n",
    "                'xli','xlii','xliii','xliv','xlv','xlvi','xlvii','xlviii','xlix','l',\n",
    "                'li','lii','liii','liv','lv','lvi','lvii','lviii','lix','lx',\n",
    "                'lxi','lxii','lxiii','lxiv','lxv','lxvi','lxvii','lxviii','lxix','lxx',\n",
    "                'lxxi','lxxii','lxxiii','lxxiv','lxxv','lxxvi','lxxvii','lxxviii','lxxix','lxxx',\n",
    "                'lxxxi','lxxxii','lxxxiii','lxxxiv','lxxxv','lxxxvi','lxxxvii','lxxxviii','lxxxix','xc',\n",
    "                'xci','xcii','xciii','xciv','xcv','xcvi','xcvii','xcviii','xcix','c']\n",
    "    if additional_stopwords:\n",
    "        stopwords.extend(additional_stopwords)\n",
    "    \n",
    "\n",
    "    def combine_words(name, stopwords):\n",
    "        # Define the words to be combined\n",
    "        words_to_combine = [\n",
    "            'sao', 'santa', 'santo', 'nova', 'novo', 'bom', 'boa', \n",
    "            'alto', 'alta', 'baixo', 'baixa', 'porto', 'campos', \n",
    "            'rio', 'foz', 'barra', 'passa', 'entre'\n",
    "        ]\n",
    "\n",
    "        # Function to replace word and its following non-stopword\n",
    "        def replace_word(match):\n",
    "            word1 = match.group(1)\n",
    "            word2 = match.group(2)\n",
    "            if word2.lower() not in stopwords:\n",
    "                return f'{word1}_{word2}'\n",
    "            return f'{word1} {word2}'\n",
    "\n",
    "        # Create a combined regex pattern for all words to be combined\n",
    "        pattern = r'\\b(' + '|'.join(words_to_combine) + r')[\\s-]+(\\w+)'\n",
    "\n",
    "        # Apply the replacement\n",
    "        name = re.sub(pattern, replace_word, name)\n",
    "\n",
    "        return name\n",
    "    \n",
    "    name = combine_words(name, stopwords)\n",
    "\n",
    "    stopwords.extend([name.lower() for name in names])\n",
    "\n",
    "    if additional_stopwords:\n",
    "        stopwords.extend(additional_stopwords)\n",
    "\n",
    "    tokens = re.split(r'\\s|,|\\.|\\(|\\)|-', name)\n",
    "    tokens = [token.strip() for token in tokens if token.strip() and token not in stopwords]\n",
    "\n",
    "    \n",
    "    # PROCESS EACH TOKEN, APPLYING SYNONYMS AND MULTI-TOKEN REPLACEMENTS\n",
    "    processed_tokens = []\n",
    "    skip_next = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        if token and token not in stopwords:\n",
    "            # HANDLE CASE WHERE 'C' IS FOLLOWED BY 'J'\n",
    "            if token == 'c' and i + 1 < len(tokens) and tokens[i + 1] == 'j':\n",
    "                processed_tokens.append('circunscricao')\n",
    "                skip_next = True\n",
    "            else:\n",
    "                processed_tokens.extend(replace_synonyms_and_multi_tokens(token))\n",
    "    \n",
    "    # REPLACE UNDERSCORES WITH SPACES IN PRESERVED CONJOINED EXPRESSIONS\n",
    "    processed_tokens = [token.replace('_', ' ') for token in processed_tokens]\n",
    "    return processed_tokens\n",
    "\n",
    "# Apply tokenization and store original names\n",
    "df['tokens'] = df['nomeUnidade'].apply(lambda x: tokenize_name(x))\n",
    "df['original_name'] = df['nomeUnidade']\n",
    "\n",
    "# Flatten list of tokens\n",
    "all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "\n",
    "# Calculate frequency of each token\n",
    "token_counts = Counter(all_tokens)\n",
    "common_tokens = token_counts.most_common()\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "token_df = pd.DataFrame(common_tokens, columns=['token', 'count'])\n",
    "\n",
    "print(f\"Number of disparate tokens: {len(set(all_tokens))}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "token_df.head(212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "# Assume 'places' is defined earlier in the code\n",
    "# places = ['sao paulo', 'rio de janeiro', 'belo horizonte', ...]  # Example, replace with actual places\n",
    "\n",
    "def get_frequent_tokens(df: pd.DataFrame, min_frequency: int = 9) -> set:\n",
    "    all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    return {token for token, count in token_counts.items() if count >= min_frequency}\n",
    "\n",
    "def assign_positions(tokens: List[str], places: List[str]) -> List[str]:\n",
    "    positions = ['alpha', 'beta', 'gamma', 'delta', 'epsilon', 'zeta', 'eta', 'theta', 'iota', 'kappa',\n",
    "                 'lambda', 'mu', 'nu', 'xi', 'omicron', 'pi', 'rho', 'sigma', 'tau', 'upsilon',\n",
    "                 'phi', 'chi', 'psi', 'omega']\n",
    "    \n",
    "    # Separate place tokens from other tokens\n",
    "    place_tokens = [token for token in tokens if token in places]\n",
    "    other_tokens = [token for token in tokens if token not in places]\n",
    "    \n",
    "    # Assign positions to other tokens\n",
    "    positioned_tokens = [f\"{token}_{positions[i]}\" for i, token in enumerate(other_tokens) if i < len(positions)]\n",
    "    \n",
    "    # Combine tokens, with place tokens at the end (without position addendum)\n",
    "    return positioned_tokens + place_tokens\n",
    "\n",
    "def initialize_entries(df: pd.DataFrame, places: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Convert DataFrame rows to Entry format with normalization and 'alien' handling,\n",
    "    considering only tokens with frequency >= 9 and adding position to token names.\n",
    "    Place tokens are replaced with '[cidade]' and always at the end of the entry without position addendum.\n",
    "    \"\"\"\n",
    "    frequent_tokens = get_frequent_tokens(df)\n",
    "    max_tokens = df['tokens'].apply(lambda x: len([token for token in x if token in frequent_tokens])).max()\n",
    "    \n",
    "    entries = []\n",
    "    for index, row in df.iterrows():\n",
    "        filtered_tokens = []\n",
    "        cidade_token = None\n",
    "        for token in row['tokens']:\n",
    "            if token in places:\n",
    "                cidade_token = '[cidade]'\n",
    "            elif token in frequent_tokens:\n",
    "                filtered_tokens.append(token)\n",
    "        \n",
    "        tokens_with_position = assign_positions(filtered_tokens, [])  # Empty list as we've already handled places\n",
    "        \n",
    "        # Add '[cidade]' token at the end if it exists\n",
    "        if cidade_token:\n",
    "            tokens_with_position.append(cidade_token)\n",
    "        \n",
    "        # Normalize the number of tokens\n",
    "        normalized_tokens = tokens_with_position + ['null'] * (max_tokens - len(tokens_with_position))\n",
    "        \n",
    "        # Check if all tokens are null, replace first with 'alien' if so\n",
    "        if all(token == 'null' for token in normalized_tokens):\n",
    "            normalized_tokens[0] = 'alien_alpha'\n",
    "        \n",
    "        entry = np.array([index, np.array(normalized_tokens, dtype=object)], dtype=object)\n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "class ClassificationStructure:\n",
    "    def __init__(self):\n",
    "        self.classifications = {}  # Dictionary to store classifications and their entry indices\n",
    "        self.entries = []  # List to store all entries\n",
    "        self.entry_classifications = {}  # Dictionary to store entry indices and their classifications\n",
    "        self.weights = {}  # Dictionary to store weights of classifications\n",
    "\n",
    "    def add_entry(self, entry: np.ndarray, classifications: List[str]):\n",
    "        entry_index = len(self.entries)\n",
    "        self.entries.append(entry)\n",
    "        self.entry_classifications[entry_index] = set()\n",
    "        \n",
    "        for classification in classifications:\n",
    "            if classification and classification != 'null':\n",
    "                if classification not in self.classifications:\n",
    "                    self.classifications[classification] = set()\n",
    "                    self.weights[classification] = 0\n",
    "                \n",
    "                self.classifications[classification].add(entry_index)\n",
    "                self.entry_classifications[entry_index].add(classification)\n",
    "                self.weights[classification] += 1\n",
    "\n",
    "    def remove_entry(self, entry_index: int):\n",
    "        if entry_index in self.entry_classifications:\n",
    "            for classification in self.entry_classifications[entry_index]:\n",
    "                self.classifications[classification].remove(entry_index)\n",
    "                self.weights[classification] -= 1\n",
    "                if len(self.classifications[classification]) == 0:\n",
    "                    del self.classifications[classification]\n",
    "                    del self.weights[classification]\n",
    "            del self.entry_classifications[entry_index]\n",
    "            self.entries[entry_index] = None\n",
    "\n",
    "    def get_entries_with_classifications(self, classifications: List[str]) -> List[np.ndarray]:\n",
    "        if not classifications:\n",
    "            return []\n",
    "        valid_sets = [self.classifications[c] for c in classifications if c in self.classifications]\n",
    "        if not valid_sets:\n",
    "            return []\n",
    "        entry_indices = set.intersection(*valid_sets)\n",
    "        return [self.entries[i] for i in entry_indices if self.entries[i] is not None]\n",
    "\n",
    "    def get_classifications_for_entry(self, entry_index: int) -> set:\n",
    "        return self.entry_classifications.get(entry_index, set())\n",
    "\n",
    "    def get_weight(self, classification: str) -> int:\n",
    "        return self.weights.get(classification, 0)\n",
    "\n",
    "    def get_overlaps(self) -> dict:\n",
    "        overlaps = {}\n",
    "        for entry_index, classifications in self.entry_classifications.items():\n",
    "            if len(classifications) > 1:\n",
    "                overlap_key = frozenset(classifications)\n",
    "                if overlap_key not in overlaps:\n",
    "                    overlaps[overlap_key] = set()\n",
    "                overlaps[overlap_key].add(entry_index)\n",
    "        return overlaps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming df is your original DataFrame)\n",
    "cs = ClassificationStructure()\n",
    "\n",
    "# Create Entry representations\n",
    "entries = initialize_entries(df, places)\n",
    "\n",
    "print(f\"Total number of entries: {len(entries)}\")\n",
    "print(\"Sample entry:\", entries[0])\n",
    "\n",
    "# Populate ClassificationStructure\n",
    "for entry in entries:\n",
    "    index, tokens = entry\n",
    "    cs.add_entry(entry, tokens)\n",
    "\n",
    "# Example queries\n",
    "print(\"\\nEntries with 'vara_alpha' (if exists):\", cs.get_entries_with_classifications(['vara_alpha']))\n",
    "print(\"\\nClassifications for entry 0:\", cs.get_classifications_for_entry(0))\n",
    "print(\"\\nWeight of 'juizado_beta' (if exists):\", cs.get_weight('juizado_beta'))\n",
    "print(\"\\nEntries with 'sao paulo' (if exists):\", cs.get_entries_with_classifications(['sao paulo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "def hierarchical_sort(entries: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    def sort_level(level_entries: List[np.ndarray], token_index: int) -> List[np.ndarray]:\n",
    "        if not level_entries or token_index >= len(level_entries[0][1]):\n",
    "            return level_entries\n",
    "\n",
    "        # Group entries by token at the current level\n",
    "        groups = defaultdict(list)\n",
    "        for entry in level_entries:\n",
    "            token = entry[1][token_index]\n",
    "            groups[token].append(entry)\n",
    "\n",
    "        # Sort groups by frequency, then alphabetically\n",
    "        sorted_groups = sorted(groups.items(), \n",
    "                               key=lambda x: (-len(x[1]), x[0] if x[0] != 'null' else 'zzz'))\n",
    "\n",
    "        # Recursively sort each group\n",
    "        sorted_entries = []\n",
    "        for _, group in sorted_groups:\n",
    "            sorted_group = sort_level(group, token_index + 1)\n",
    "            sorted_entries.extend(sorted_group)\n",
    "\n",
    "        return sorted_entries\n",
    "\n",
    "    return sort_level(entries, 0)\n",
    "\n",
    "# Assuming 'entries' is your list of entry arrays\n",
    "sorted_entries = hierarchical_sort(entries)\n",
    "\n",
    "# Print the first few sorted entries to verify\n",
    "for entry in sorted_entries:\n",
    "    print(f\"Index: {entry[0]}, Tokens: {entry[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "class TaxonomicNode:\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "        self.children: Dict[str, TaxonomicNode] = {}\n",
    "        self.entries: List[int] = []\n",
    "\n",
    "def build_taxonomic_tree(sorted_entries: List[np.ndarray]) -> TaxonomicNode:\n",
    "    root = TaxonomicNode(\"root\")\n",
    "    max_depth = len(sorted_entries[0][1])  # Assuming all entries have the same length\n",
    "\n",
    "    for entry in sorted_entries:\n",
    "        entry_index, tokens = entry\n",
    "        current_node = root\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == 'null':\n",
    "                break\n",
    "            if token not in current_node.children:\n",
    "                current_node.children[token] = TaxonomicNode(token)\n",
    "            current_node = current_node.children[token]\n",
    "        \n",
    "        current_node.entries.append(entry_index)\n",
    "\n",
    "    normalize_tree_depth(root, max_depth)\n",
    "    return root\n",
    "\n",
    "def normalize_tree_depth(node: TaxonomicNode, target_depth: int, current_depth: int = 0):\n",
    "    if current_depth == target_depth - 1:\n",
    "        return\n",
    "\n",
    "    if not node.children:\n",
    "        for i in range(current_depth, target_depth - 1):\n",
    "            dummy_name = f\"{node.value}_{'abcdefghijklmnopqrstuvwxyz'[i-current_depth]}\"\n",
    "            node.children[dummy_name] = TaxonomicNode(dummy_name)\n",
    "            node = node.children[dummy_name]\n",
    "    else:\n",
    "        for child in node.children.values():\n",
    "            normalize_tree_depth(child, target_depth, current_depth + 1)\n",
    "\n",
    "def print_tree(node: TaxonomicNode, depth: int = 0):\n",
    "    print(\"  \" * depth + node.value)\n",
    "    if node.entries:\n",
    "        print(\"  \" * (depth + 1) + f\"Entries: {node.entries}\")\n",
    "    for child in node.children.values():\n",
    "        print_tree(child, depth + 1)\n",
    "\n",
    "# Assuming 'sorted_entries' is your list of sorted entry arrays\n",
    "taxonomic_tree = build_taxonomic_tree(sorted_entries)\n",
    "\n",
    "# Print the tree to verify\n",
    "print_tree(taxonomic_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Assuming `sorted_entries` is the final list of entries to export\n",
    "with open('tree_output.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Index\", \"Tokens\"])  # Write header, adjust as necessary\n",
    "    for entry in sorted_entries:\n",
    "        writer.writerow([entry[0], \", \".join(entry[1])])  # Convert token list to string for CSV\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
